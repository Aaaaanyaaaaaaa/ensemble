{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45cdf971-1040-41ed-8030-e6742fbf5c9c",
   "metadata": {},
   "source": [
    "# LOAD LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac474a-06de-42f0-a74f-d04e387e55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba5bf66-7251-4199-8e09-f58a272e18a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import PIL\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb332d4-5dcd-49b0-aebc-6aee29885891",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647691c9-043c-465e-9138-50ed473920b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# import datasets\n",
    "import utils.misc as misc\n",
    "from utils.box_utils import xywh2xyxy, bbox_iou, calculate_iou_mask, calculate_dice_mask\n",
    "from utils.visual_bbox import visualBBox\n",
    "from models import build_model\n",
    "import datasets.transforms as T\n",
    "import PIL.Image as Image\n",
    "import data_loader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "# get_args_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01421871-1335-42a9-9e26-cff21b9dea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d0e38-3ba1-4acc-8051-80e3d7ff992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device= \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61a5f46-6ac3-470c-b040-059f618c718e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_500 = pd.read_csv('gold_500.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3519060c-e6a5-4204-a614-8db908154287",
   "metadata": {},
   "source": [
    "# LOAD MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2597adc-cebe-464a-ae96-85b8bb70b694",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lr_bert', default=0., type=float)\n",
    "parser.add_argument('--lr_visu_cnn', default=0., type=float)\n",
    "parser.add_argument('--lr_visu_tra', default=1e-5, type=float)\n",
    "parser.add_argument('--batch_size', default=32, type=int)\n",
    "parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "parser.add_argument('--epochs', default=100, type=int)\n",
    "parser.add_argument('--lr_power', default=0.9, type=float, help='lr poly power')\n",
    "parser.add_argument('--clip_max_norm', default=0., type=float,\n",
    "                    help='gradient clipping max norm')\n",
    "parser.add_argument('--eval', dest='eval', default=False, action='store_true', help='if evaluation only')\n",
    "parser.add_argument('--optimizer', default='rmsprop', type=str)\n",
    "parser.add_argument('--lr_scheduler', default='poly', type=str)\n",
    "parser.add_argument('--lr_drop', default=80, type=int)\n",
    "# Model parameters\n",
    "parser.add_argument('--model_name', type=str, default='TransVG_ca',\n",
    "                    help=\"Name of model to be exploited.\")\n",
    "\n",
    "\n",
    "# Transformers in two branches\n",
    "parser.add_argument('--bert_enc_num', default=12, type=int)\n",
    "parser.add_argument('--detr_enc_num', default=6, type=int)\n",
    "\n",
    "# DETR parameters\n",
    "# * Backbone\n",
    "parser.add_argument('--backbone', default='resnet50', type=str,\n",
    "                    help=\"Name of the convolutional backbone to use\")\n",
    "parser.add_argument('--dilation', action='store_true',\n",
    "                    help=\"If true, we replace stride with dilation in the last convolutional block (DC5)\")\n",
    "parser.add_argument('--position_embedding', default='sine', type=str, choices=('sine', 'learned'), help=\"Type of positional embedding to use on top of the image features\")\n",
    "# * Transformer\n",
    "parser.add_argument('--enc_layers', default=6, type=int,\n",
    "                    help=\"Number of encoding layers in the transformer\")\n",
    "parser.add_argument('--dec_layers', default=0, type=int,\n",
    "                    help=\"Number of decoding layers in the transformer\")\n",
    "parser.add_argument('--dim_feedforward', default=2048, type=int,\n",
    "                    help=\"Intermediate size of the feedforward layers in the transformer blocks\")\n",
    "parser.add_argument('--hidden_dim', default=256, type=int,\n",
    "                    help=\"Size of the embeddings (dimension of the transformer)\")\n",
    "parser.add_argument('--dropout', default=0.1, type=float,\n",
    "                    help=\"Dropout applied in the transformer\")\n",
    "parser.add_argument('--nheads', default=8, type=int,\n",
    "                    help=\"Number of attention heads inside the transformer's attentions\")\n",
    "parser.add_argument('--num_queries', default=100, type=int,\n",
    "                    help=\"Number of query slots\")\n",
    "parser.add_argument('--pre_norm', action='store_true')\n",
    "\n",
    "parser.add_argument('--imsize', default=640, type=int, help='image size')\n",
    "parser.add_argument('--emb_size', default=512, type=int,\n",
    "                    help='fusion module embedding dimensions')\n",
    "# Vision-Language Transformer\n",
    "parser.add_argument('--use_vl_type_embed', action='store_true',\n",
    "                    help=\"If true, use vl_type embedding\")\n",
    "parser.add_argument('--vl_dropout', default=0.1, type=float,\n",
    "                    help=\"Dropout applied in the vision-language transformer\")\n",
    "parser.add_argument('--vl_nheads', default=8, type=int,\n",
    "                    help=\"Number of attention heads inside the vision-language transformer's attentions\")\n",
    "parser.add_argument('--vl_hidden_dim', default=256, type=int,\n",
    "                    help='Size of the embeddings (dimension of the vision-language transformer)')\n",
    "parser.add_argument('--vl_dim_feedforward', default=2048, type=int,\n",
    "                    help=\"Intermediate size of the feedforward layers in the vision-language transformer blocks\")\n",
    "parser.add_argument('--vl_enc_layers', default=6, type=int,\n",
    "                    help='Number of encoders in the vision-language transformer')\n",
    "\n",
    "\n",
    "parser.add_argument('--dataset', default='MS_CXR', type=str,\n",
    "                    help='referit/flickr/unc/unc+/gref')\n",
    "parser.add_argument('--max_query_len', default=20, type=int,\n",
    "                    help='maximum time steps (lang length) per batch')\n",
    "\n",
    "# dataset parameters\n",
    "parser.add_argument('--output_dir', default='answers',\n",
    "                    help='path where to save, empty for no saving')\n",
    "parser.add_argument('--device', default='cuda',\n",
    "                    help='device to use for training / testing')\n",
    "\n",
    "parser.add_argument('--detr_model', default='./saved_models/detr-r50.pth', type=str, help='detr model')\n",
    "parser.add_argument('--bert_model', default='bert-base-uncased', type=str, help='bert model')\n",
    "\n",
    "parser.add_argument('--eval_model', default='released_checkpoint/MedMPG_MS_CXR.pth', type=str)\n",
    "\n",
    "parser.add_argument('--body_part', default='cardiac silhouette', type=str)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdde86d-6833-4bd9-b0a4-ec38c2a72ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "args, unknown = parser.parse_known_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef444e-7abf-4589-af05-1b7dbac690b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0205a801-6e06-415f-ad96-b2c73e7b54af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to('cuda')\n",
    "checkpoint = torch.load(args.eval_model, map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba58d5f-cbf7-44a7-9592-8de4522f1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_transforms(imsize):\n",
    "    \"\"\"\n",
    "    image transformations\n",
    "    \"\"\"\n",
    "    return T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.NormalizeAndPad(size=imsize),\n",
    "        ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5374d5d8-0f0e-422e-9b23-195156b562e7",
   "metadata": {},
   "source": [
    "# NECESSARY DICT AND FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b611e4b-72dd-4536-9782-1d4947dff9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the folder for all the directories\n",
    "!mkdir \"<YOUR_FOLDER_NAME>\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e9015d-0f83-437f-a8c1-fbecde7e667d",
   "metadata": {},
   "outputs": [],
   "source": [
    "body_parts = gold_500.bbox_name.unique()\n",
    "\n",
    "# the structure of thisdictionary is part--> \"{image_name --> coordinates of the bounding box}\"\n",
    "\n",
    "#this one is for the original body box\n",
    "bboxes_orig = {} \n",
    "for part in body_parts:\n",
    "    bboxes_orig[part] = {}\n",
    "\n",
    "#this one is for the maira predicted body box\n",
    "bboxes_medRPG = {} \n",
    "for part in body_parts:\n",
    "    bboxes_medRPG[part] = {}\n",
    "\n",
    "# the structure of thisdictionary is part--> \"{image_name --> iou of the predicted bbox relating to original bbox}\"\n",
    "bboxes_iou_medrpg = {} \n",
    "for part in body_parts:\n",
    "    bboxes_iou_medrpg[part] = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a34fb9-c2e8-47e3-8203-32b5b24b93d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou_mask(predictions, targets, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    calculate iou of two masks (predicted and original)\n",
    "    \"\"\"\n",
    "    predictions = predictions.byte()\n",
    "    targets = targets.byte()\n",
    "    intersection = (predictions & targets).sum((0, 1))\n",
    "    union = (predictions | targets).sum((0, 1))\n",
    "    iou = (intersection + epsilon) / (union + epsilon)\n",
    "    return iou\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6a3738-d0c9-4c4e-a6f2-1505842f915b",
   "metadata": {},
   "source": [
    "# GETTING THE MODEL'S PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f03680-7934-4a32-93ed-3c8e118557c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from tqdm import tqdm\n",
    "\n",
    "for part in body_parts:\n",
    "   \n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    parent_dir = \"<YOUR_FOLDER_NAME>\"\n",
    "    path = os.path.join(parent_dir, part) \n",
    "    os.mkdir(path) \n",
    "    s = 0\n",
    "    iou = 0\n",
    "    iou1 = 0\n",
    "    \n",
    "    \n",
    "    for image in tqdm(gold_500.image_id_jpg.unique()):\n",
    "        path1 = \"<path to resized mimic>/\"\n",
    "        path2 = \"<path to originalsized mimic>\"\n",
    "        img_path = path1 + image\n",
    "        size_path = path2 + image\n",
    "       \n",
    "        if (os.path.exists(img_path) and os.path.exists(size_path)):\n",
    "            \n",
    "            if len(gold_500.original_x1[((gold_500.image_id_jpg==image) & (gold_500.bbox_name ==part))].values) > 0:\n",
    "                W, H = Image.open(size_path).size\n",
    "                img_try = Image.open(img_path).convert(\"RGB\").resize((640, 640 ))\n",
    "                x = gold_500.original_x1[((gold_500.image_id_jpg==image) & (gold_500.bbox_name ==part))].values[0]\n",
    "                y = gold_500.original_y1[((gold_500.image_id_jpg==image) & (gold_500.bbox_name ==part))].values[0]\n",
    "                w = gold_500.original_width[((gold_500.image_id_jpg==image) & (gold_500.bbox_name ==part))].values[0]\n",
    "                h = gold_500.original_height[((gold_500.image_id_jpg==image) & (gold_500.bbox_name ==part))].values[0]\n",
    "                bbox = [round(x / W *500) , round(y / H *500), round(w / W *500), round(h / H *500)]\n",
    "\n",
    "\n",
    "                \n",
    "                examples = data_loader.read_examples(part, 1)\n",
    "                tokenizer = AutoTokenizer.from_pretrained(args.bert_model, do_lower_case=True)\n",
    "                features = data_loader.convert_examples_to_features(\n",
    "                    examples=examples, seq_length=args.max_query_len, tokenizer=tokenizer, usemarker=None)\n",
    "                word_id = torch.tensor(features[0].input_ids)  #\n",
    "                word_mask = torch.tensor(features[0].input_mask)\n",
    "\n",
    "                input_dict = dict()\n",
    "                input_dict['img'] = img_try\n",
    "                fake_bbox = torch.tensor(np.array([0,0,0,0], dtype=int)).float() \n",
    "                input_dict['box'] = fake_bbox \n",
    "                input_dict['text'] = part\n",
    "                transform = make_transforms(imsize=640)\n",
    "                input_dict = transform(input_dict)\n",
    "                img = input_dict['img']  \n",
    "                img_mask = input_dict['mask'] \n",
    "\n",
    "                img_data = misc.NestedTensor(img.unsqueeze(0), img_mask.unsqueeze(0))\n",
    "                text_data = misc.NestedTensor(word_id.unsqueeze(0), word_mask.unsqueeze(0))\n",
    "                img_data = img_data.to(device)\n",
    "                text_data = text_data.to(device)\n",
    "                \n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    outputs = model(img_data, text_data)\n",
    "                    pred_box = outputs['pred_box']\n",
    "                    \n",
    "                    pred_box = pred_box.detach().cpu().numpy()[0] *500\n",
    "                    pred_box = [round(pred_box[0]), round(pred_box[1]), \n",
    "                                round(pred_box[2]), round(pred_box[3])]\n",
    "                    \n",
    "                    # predicted coordinates are the coordinattes of center, width and height. \n",
    "                    #We need to transform them into lower left angle, width, height\n",
    "                    x_p, y_p, w_p, h_p = pred_box\n",
    "                    x_p = round(x_p-0.5*w_p)\n",
    "                    y_p = round(y_p-0.5*h_p)\n",
    "\n",
    "                    bboxes_medRPG[part][image] = (x_p, y_p, w_p, h_p)\n",
    "\n",
    "                \n",
    "                #original bbox\n",
    "                x_o,y_o, w_o, h_o = bbox\n",
    "                bboxes_orig[part][image] = (x_o, y_o, w_o, h_o)   \n",
    "                \n",
    "                \n",
    "                fig, ax = plt.subplots()\n",
    "                #changing the coordinate grid\n",
    "                ax.imshow(np.flipud(Image.open(size_path).resize((500, 500))), origin='lower')\n",
    "                \n",
    "\n",
    "                #reflect the bboxes\n",
    "                rect2 = patches.Rectangle((x_p,   500 - y_p), w_p, -h_p, linewidth=3, edgecolor='red', facecolor='none')\n",
    "                ax.add_patch(rect2)\n",
    "\n",
    "                \n",
    "                rect3 = patches.Rectangle((x_o,  500 - y_o), w_o, -h_o, linewidth=3, edgecolor='yellow', facecolor='none')\n",
    "                ax.add_patch(rect3)\n",
    "             \n",
    "                # plt.show()\n",
    "                path_image = os.path.join(path, image) \n",
    "                fig.savefig(path_image)\n",
    "                \n",
    "                \n",
    "                W, H = 500, 500\n",
    "                mask_orig = torch.tensor(np.zeros((W,H)))\n",
    "                mask_orig[ x_o: x_o+w_o, y_o:y_o+h_o] = 1\n",
    "                \n",
    "                \n",
    "                mask_pred = torch.tensor(np.zeros((W,H)))\n",
    "                mask_pred[ x_p: x_p+w_p, y_p:y_p+h_p] = 1\n",
    "                \n",
    "               \n",
    "                bboxes_iou_medrpg[part][image] = calculate_iou_mask(mask_pred, mask_orig).item()\n",
    "\n",
    "\n",
    "    with open('<NAME>.json', 'w') as json_file:\n",
    "        json.dump(bboxes_orig , json_file, allow_nan=True) #ground truth bboxes\n",
    "\n",
    "    with open('<NAME>.json', 'w') as json_file:\n",
    "        json.dump(bboxes_medRPG, json_file, allow_nan=True) #MedRPG predicted bboxed\n",
    "\n",
    "    with open('IOU_MedRPG_v2.json', 'w') as json_file:\n",
    "        json.dump(bboxes_iou_medrpg, json_file, allow_nan=True) #iou computed for every picture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6c3c0-ff51-49af-be66-3e6d21e735f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MedRPG",
   "language": "python",
   "name": "medrpg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
